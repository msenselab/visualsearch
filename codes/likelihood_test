'''
May 2018
tests likelihood function
'''

import sys
import itertools as it
import seaborn as sns
from scipy.optimize import brentq
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation, writers
from scipy.stats import gaussian_kde, norm
import pandas as pd
from pathlib import Path
import pickle
from gauss_opt import bayesian_optimisation
from dynamic_adhoc_twosigma import posterior
import numpy as np
from dyn_case_multi_model import get_coarse_stats, trans_probs, solve_rho, back_induct

def simulate_observer(arglist):
    C, decisions, sigma, mu, dt = arglist

    dec_vec = decisions[:,0]
    abs_bound = g_values[np.amax(np.where(dec_vec == 1)[0])]
    pres_bound = g_values[np.where(dec_vec == 2)[0][0]]

    D_t = 0
    t = 0

    g_trajectory = np.ones(int(T/dt))*0.5
    D_trajectory = np.zeros(int(T/dt))

    while t < T:
        if C == 1:
            D_t = norm.rvs(t * mu[C], t* sigma[C]) * dt
        if C == 0:
            D_t = norm.rvs(t * mu[C], t*sigma[C]) * dt

        g_t = D_to_g(D_t)
        D_trajectory[int(t/dt)] = D_t
        g_trajectory[int(t/dt)] = g_t
        t += dt

        if g_t < abs_bound:
            return (1, t, 1 == C)

        if g_t > pres_bound:
            return (2, t, 2 == C)

    return (0, T, 0==C)


def get_rt(sigma, mu, decisions, numsims = 5000):
    C_vals = [0] * numsims
    C_vals.extend([1] * numsims)
    arglists = it.product(C_vals, [decisions], [sigma], [mu], [dt])
    observer_outputs = []
    for arglist in arglists:
        observer_outputs.append(simulate_observer(arglist))
    return observer_outputs

def gen_data(params, model_type):
    numsims = 2000
    if model_type[0] == 'sig':
        reward = 1
        punishment = 0
        fine_sigma = params[0]
    if model_type[0] == 'sig_reward':
        reward = params[1]
        punishment = 0
        fine_sigma = params[0]
    if model_type[0] == 'sig_punish':
        reward = 1
        punishment = params[1]
        fine_sigma = params[0]

    stats = get_coarse_stats(fine_sigma, d_map_samples, fine_model_type)
    virutal_data = np.zeros((stats.shape[0], numsims))
    for i in range(stats.shape[0]):
        mu = stats[i, :, 0]
        sigma = stats[i, :, 1]
        probs = trans_probs(sigma, mu)
        rho = solve_rho(reward, punishment, reward_scheme, sigma, mu, probs)
        decisions = back_induct(reward, punishment, rho, sigma, mu,
                                                probs, reward_scheme)[1]
        sim_rt = get_rt(sigma, mu, decisions, numsims)

        virutal_data[i, :] = sim_rt[0,:]


    return virutal_data


def test_get_single_N_likelihood(data, sim_rt, reward):

    pres_rts_0 = data.query('resp == 2 & target == \'Present\'').rt.values
    pres_rts_1 = data.query('resp == 1 & target == \'Present\'').rt.values

    abs_rts_0 = data.query('resp == 2 & target == \'Absent\'').rt.values
    abs_rts_1 = data.query('resp == 1 & target == \'Absent\'').rt.values

    # deal with the cases where sim rt are all the same giving 0 var to KDE
    if np.var(sim_rt[1, :]) == 0:
        mean = np.mean(sim_rt[1, :])
        perturb = np.random.normal(mean, 0.01)
        sim_rt[1, 0] = mean + perturb

    pres_sim_rt_dist = gaussian_kde(sim_rt[1, :], bw_method=0.1)

    if np.var(sim_rt[0, :]) == 0:
        mean = np.mean(sim_rt[0, :])
        perturb = np.random.normal(mean, 0.1)
        sim_rt[0, 0] = mean + perturb

    abs_sim_rt_dist = gaussian_kde(sim_rt[0, :], bw_method=0.1)

    frac_pres_inc = len(pres_rts_0) / (len(pres_rts_0) + len(pres_rts_1))
    frac_pres_corr = len(pres_rts_1) / (len(pres_rts_0) + len(pres_rts_1))
    log_like_pres = np.concatenate((np.log(frac_pres_inc) +
                                    np.log(pres_sim_rt_dist.pdf(pres_rts_0)),
                                    np.log(frac_pres_corr) +
                                    np.log(pres_sim_rt_dist.pdf(pres_rts_1))))

    frac_abs_inc = len(abs_rts_1) / (len(abs_rts_0) + len(abs_rts_1))
    frac_abs_corr = len(abs_rts_0) / (len(abs_rts_0) + len(abs_rts_1))
    log_like_abs = np.concatenate((np.log(frac_abs_corr) +
                                   np.log(abs_sim_rt_dist.pdf(abs_rts_0)),
                                   np.log(frac_abs_inc) +
                                   np.log(abs_sim_rt_dist.pdf(abs_rts_1))))

    log_like_all = np.concatenate((log_like_pres, log_like_abs))

    likelihood_pertrial = (1 - lapse) * np.exp(log_like_all) + (lapse / 2) * np.exp(-reward / temp)
    return -np.sum(np.log(likelihood_pertrial))



def test_get_data_likelihood(gen_data, log_reward, log_punishment, log_sigma,
                                        reward_scheme, fine_model_type):
    sigma = np.exp(log_sigma)
    reward = np.exp(log_reward)
    punishment = -np.exp(log_punishment)
    print(sigma, reward, punishment)
    likelihood = 0
    data = [sub_data.query('setsize == 8'), sub_data.query('setsize == 12'),
            sub_data.query('setsize == 16')]

    stats = get_coarse_stats(sigma, d_map_samples, fine_model_type)

    for i in range(stats.shape[0]):
        mu = stats[i, :, 0]
        sigma = stats[i, :, 1]
        probs = trans_probs(sigma, mu)
        rho = solve_rho(reward, punishment, reward_scheme, sigma, mu, probs)
        decisions = back_induct(reward, punishment, rho, sigma, mu,
                                                probs, reward_scheme)[1]
        sim_rt = get_rt(sigma, mu, decisions)
        likelihood += get_single_N_likelihood(data[i], sim_rt, reward)

    return likelihood
